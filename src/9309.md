  RFC 9309         Robots Exclusion Protocol (REP)   September 2022
  ---------------- --------------------------------- ----------------
  Koster, et al.   Standards Track                   \[Page\]

::: {#external-metadata .document-information}
:::

::: {#internal-metadata .document-information}

Stream:
:   Internet Engineering Task Force (IETF)

RFC:
:   [9309](https://www.rfc-editor.org/rfc/rfc9309){.eref}

Category:
:   Standards Track

Published:
:   September 2022

ISSN:
:   2070-1721

Authors:

:   ::: author
    ::: author-name
    M. Koster
    :::
    :::

    ::: author
    ::: author-name
    G. Illyes
    :::

    ::: org
    Google LLC
    :::
    :::

    ::: author
    ::: author-name
    H. Zeller
    :::

    ::: org
    Google LLC
    :::
    :::

    ::: author
    ::: author-name
    L. Sassman
    :::

    ::: org
    Google LLC
    :::
    :::
:::

# RFC 9309 {#rfcnum}

# Robots Exclusion Protocol {#title}

::: {#section-abstract .section}
## [Abstract](#abstract){.selfRef}

This document specifies and extends the \"Robots Exclusion Protocol\"
method originally defined by Martijn Koster in 1994 for service owners
to control how content served by their services may be accessed, if at
all, by automatic clients known as crawlers. Specifically, it adds
definition language for the protocol, instructions for handling errors,
and instructions for caching.[¶](#section-abstract-1){.pilcrow}
:::

::: {#status-of-memo}
::: {#section-boilerplate.1 .section}
## [Status of This Memo](#name-status-of-this-memo){.section-name .selfRef} {#name-status-of-this-memo}

This is an Internet Standards Track
document.[¶](#section-boilerplate.1-1){.pilcrow}

This document is a product of the Internet Engineering Task Force
(IETF). It represents the consensus of the IETF community. It has
received public review and has been approved for publication by the
Internet Engineering Steering Group (IESG). Further information on
Internet Standards is available in Section 2 of RFC
7841.[¶](#section-boilerplate.1-2){.pilcrow}

Information about the current status of this document, any errata, and
how to provide feedback on it may be obtained at
<https://www.rfc-editor.org/info/rfc9309>.[¶](#section-boilerplate.1-3){.pilcrow}
:::
:::

::: {#copyright}
::: {#section-boilerplate.2 .section}
## [Copyright Notice](#name-copyright-notice){.section-name .selfRef} {#name-copyright-notice}

Copyright (c) 2022 IETF Trust and the persons identified as the document
authors. All rights reserved.[¶](#section-boilerplate.2-1){.pilcrow}

This document is subject to BCP 78 and the IETF Trust\'s Legal
Provisions Relating to IETF Documents
(<https://trustee.ietf.org/license-info>) in effect on the date of
publication of this document. Please review these documents carefully,
as they describe your rights and restrictions with respect to this
document. Code Components extracted from this document must include
Revised BSD License text as described in Section 4.e of the Trust Legal
Provisions and are provided without warranty as described in the Revised
BSD License.[¶](#section-boilerplate.2-2){.pilcrow}
:::
:::

::: {#toc}
::: {#section-toc.1 .section}
[▲](#){.toplink}

## [Table of Contents](#name-table-of-contents){.section-name .selfRef} {#name-table-of-contents}

-   ::: {#section-toc.1-1.1}
    [1](#section-1){.xref}.  [Introduction](#name-introduction){.xref}

    -   ::: {#section-toc.1-1.1.2.1}
        [1.1](#section-1.1){.xref}.  [Requirements
        Language](#name-requirements-language){.xref}
        :::
    :::

-   ::: {#section-toc.1-1.2}
    [2](#section-2){.xref}.  [Specification](#name-specification){.xref}

    -   ::: {#section-toc.1-1.2.2.1}
        [2.1](#section-2.1){.xref}.  [Protocol
        Definition](#name-protocol-definition){.xref}
        :::

    -   ::: {#section-toc.1-1.2.2.2}
        [2.2](#section-2.2){.xref}.  [Formal
        Syntax](#name-formal-syntax){.xref}

        -   ::: {#section-toc.1-1.2.2.2.2.1}
            [2.2.1](#section-2.2.1){.xref}.  [The User-Agent
            Line](#name-the-user-agent-line){.xref}
            :::

        -   ::: {#section-toc.1-1.2.2.2.2.2}
            [2.2.2](#section-2.2.2){.xref}.  [The \"Allow\" and
            \"Disallow\"
            Lines](#name-the-allow-and-disallow-line){.xref}
            :::

        -   ::: {#section-toc.1-1.2.2.2.2.3}
            [2.2.3](#section-2.2.3){.xref}.  [Special
            Characters](#name-special-characters){.xref}
            :::

        -   ::: {#section-toc.1-1.2.2.2.2.4}
            [2.2.4](#section-2.2.4){.xref}.  [Other
            Records](#name-other-records){.xref}
            :::
        :::

    -   ::: {#section-toc.1-1.2.2.3}
        [2.3](#section-2.3){.xref}.  [Access
        Method](#name-access-method){.xref}

        -   ::: {#section-toc.1-1.2.2.3.2.1}
            [2.3.1](#section-2.3.1){.xref}.  [Access
            Results](#name-access-results){.xref}

            -   ::: {#section-toc.1-1.2.2.3.2.1.2.1}
                [2.3.1.1](#section-2.3.1.1){.xref}.  [Successful
                Access](#name-successful-access){.xref}
                :::

            -   ::: {#section-toc.1-1.2.2.3.2.1.2.2}
                [2.3.1.2](#section-2.3.1.2){.xref}.  [Redirects](#name-redirects){.xref}
                :::

            -   ::: {#section-toc.1-1.2.2.3.2.1.2.3}
                [2.3.1.3](#section-2.3.1.3){.xref}.  [\"Unavailable\"
                Status](#name-unavailable-status){.xref}
                :::

            -   ::: {#section-toc.1-1.2.2.3.2.1.2.4}
                [2.3.1.4](#section-2.3.1.4){.xref}.  [\"Unreachable\"
                Status](#name-unreachable-status){.xref}
                :::

            -   ::: {#section-toc.1-1.2.2.3.2.1.2.5}
                [2.3.1.5](#section-2.3.1.5){.xref}.  [Parsing
                Errors](#name-parsing-errors){.xref}
                :::
            :::
        :::

    -   ::: {#section-toc.1-1.2.2.4}
        [2.4](#section-2.4){.xref}.  [Caching](#name-caching){.xref}
        :::

    -   ::: {#section-toc.1-1.2.2.5}
        [2.5](#section-2.5){.xref}.  [Limits](#name-limits){.xref}
        :::
    :::

-   ::: {#section-toc.1-1.3}
    [3](#section-3){.xref}.  [Security
    Considerations](#name-security-considerations){.xref}
    :::

-   ::: {#section-toc.1-1.4}
    [4](#section-4){.xref}.  [IANA
    Considerations](#name-iana-considerations){.xref}
    :::

-   ::: {#section-toc.1-1.5}
    [5](#section-5){.xref}.  [Examples](#name-examples){.xref}

    -   ::: {#section-toc.1-1.5.2.1}
        [5.1](#section-5.1){.xref}.  [Simple
        Example](#name-simple-example){.xref}
        :::

    -   ::: {#section-toc.1-1.5.2.2}
        [5.2](#section-5.2){.xref}.  [Longest
        Match](#name-longest-match){.xref}
        :::
    :::

-   ::: {#section-toc.1-1.6}
    [6](#section-6){.xref}.  [References](#name-references){.xref}

    -   ::: {#section-toc.1-1.6.2.1}
        [6.1](#section-6.1){.xref}.  [Normative
        References](#name-normative-references){.xref}
        :::

    -   ::: {#section-toc.1-1.6.2.2}
        [6.2](#section-6.2){.xref}.  [Informative
        References](#name-informative-references){.xref}
        :::
    :::

-   ::: {#section-toc.1-1.7}
    [](#appendix-A){.xref}[Authors\'
    Addresses](#name-authors-addresses){.xref}
    :::
:::
:::

::: {#introduction}
::: {#section-1 .section}
## [1.](#section-1){.section-number .selfRef} [Introduction](#name-introduction){.section-name .selfRef} {#name-introduction}

This document applies to services that provide resources that clients
can access through URIs as defined in \[[RFC3986](#RFC3986){.xref}\].
For example, in the context of HTTP, a browser is a client that displays
the content of a web page.[¶](#section-1-1){.pilcrow}

Crawlers are automated clients. Search engines, for instance, have
crawlers to recursively traverse links for indexing as defined in
\[[RFC8288](#RFC8288){.xref}\].[¶](#section-1-2){.pilcrow}

It may be inconvenient for service owners if crawlers visit the entirety
of their URI space. This document specifies the rules originally defined
by the \"Robots Exclusion Protocol\" \[[ROBOTSTXT](#ROBOTSTXT){.xref}\]
that crawlers are requested to honor when accessing
URIs.[¶](#section-1-3){.pilcrow}

These rules are not a form of access
authorization.[¶](#section-1-4){.pilcrow}

::: {#requirements-language}
::: {#section-1.1 .section}
### [1.1.](#section-1.1){.section-number .selfRef} [Requirements Language](#name-requirements-language){.section-name .selfRef} {#name-requirements-language}

The key words \"[MUST]{.bcp14}\", \"[MUST NOT]{.bcp14}\",
\"[REQUIRED]{.bcp14}\", \"[SHALL]{.bcp14}\", \"[SHALL NOT]{.bcp14}\",
\"[SHOULD]{.bcp14}\", \"[SHOULD NOT]{.bcp14}\",
\"[RECOMMENDED]{.bcp14}\", \"[NOT RECOMMENDED]{.bcp14}\",
\"[MAY]{.bcp14}\", and \"[OPTIONAL]{.bcp14}\" in this document are to be
interpreted as described in BCP 14 \[[RFC2119](#RFC2119){.xref}\]
\[[RFC8174](#RFC8174){.xref}\] when, and only when, they appear in all
capitals, as shown here.[¶](#section-1.1-1){.pilcrow}
:::
:::
:::
:::

::: {#specification}
::: {#section-2 .section}
## [2.](#section-2){.section-number .selfRef} [Specification](#name-specification){.section-name .selfRef} {#name-specification}

::: {#protocol-definition}
::: {#section-2.1 .section}
### [2.1.](#section-2.1){.section-number .selfRef} [Protocol Definition](#name-protocol-definition){.section-name .selfRef} {#name-protocol-definition}

The protocol language consists of rule(s) and group(s) that the service
makes available in a file named \"robots.txt\" as described in [Section
2.3](#access-method){.xref}:[¶](#section-2.1-1){.pilcrow}

[]{.break}

Rule:
:   A line with a key-value pair that defines how a crawler may access
    URIs. See [Section
    2.2.2](#the-allow-and-disallow-lines){.xref}.[¶](#section-2.1-2.2){.pilcrow}
:   

Group:
:   One or more user-agent lines that are followed by one or more rules.
    The group is terminated by a user-agent line or end of file. See
    [Section 2.2.1](#the-user-agent-line){.xref}. The last group may
    have no rules, which means it implicitly allows
    everything.[¶](#section-2.1-2.4){.pilcrow}
:   
:::
:::

::: {#formal-syntax}
::: {#section-2.2 .section}
### [2.2.](#section-2.2){.section-number .selfRef} [Formal Syntax](#name-formal-syntax){.section-name .selfRef} {#name-formal-syntax}

Below is an Augmented Backus-Naur Form (ABNF) description, as described
in \[[RFC5234](#RFC5234){.xref}\].[¶](#section-2.2-1){.pilcrow}

::: {#section-2.2-2}
``` {.lang-abnf .sourcecode}
 robotstxt = *(group / emptyline)
 group = startgroupline                ; We start with a user-agent
                                       ; line
        *(startgroupline / emptyline)  ; ... and possibly more
                                       ; user-agent lines
        *(rule / emptyline)            ; followed by rules relevant
                                       ; for the preceding
                                       ; user-agent lines

 startgroupline = *WS "user-agent" *WS ":" *WS product-token EOL

 rule = *WS ("allow" / "disallow") *WS ":"
       *WS (path-pattern / empty-pattern) EOL

 ; parser implementors: define additional lines you need (for
 ; example, Sitemaps).

 product-token = identifier / "*"
 path-pattern = "/" *UTF8-char-noctl ; valid URI path pattern
 empty-pattern = *WS

 identifier = 1*(%x2D / %x41-5A / %x5F / %x61-7A)
 comment = "#" *(UTF8-char-noctl / WS / "#")
 emptyline = EOL
 EOL = *WS [comment] NL ; end-of-line may have
                        ; optional trailing comment
 NL = %x0D / %x0A / %x0D.0A
 WS = %x20 / %x09

 ; UTF8 derived from RFC 3629, but excluding control characters

 UTF8-char-noctl = UTF8-1-noctl / UTF8-2 / UTF8-3 / UTF8-4
 UTF8-1-noctl = %x21 / %x22 / %x24-7F ; excluding control, space, "#"
 UTF8-2 = %xC2-DF UTF8-tail
 UTF8-3 = %xE0 %xA0-BF UTF8-tail / %xE1-EC 2UTF8-tail /
          %xED %x80-9F UTF8-tail / %xEE-EF 2UTF8-tail
 UTF8-4 = %xF0 %x90-BF 2UTF8-tail / %xF1-F3 3UTF8-tail /
          %xF4 %x80-8F 2UTF8-tail

 UTF8-tail = %x80-BF
```

[¶](#section-2.2-2){.pilcrow}
:::

::: {#the-user-agent-line}
::: {#section-2.2.1 .section}
#### [2.2.1.](#section-2.2.1){.section-number .selfRef} [The User-Agent Line](#name-the-user-agent-line){.section-name .selfRef} {#name-the-user-agent-line}

Crawlers set their own name, which is called a product token, to find
relevant groups. The product token [MUST]{.bcp14} contain only uppercase
and lowercase letters (\"a-z\" and \"A-Z\"), underscores (\"\_\"), and
hyphens (\"-\"). The product token [SHOULD]{.bcp14} be a substring of
the identification string that the crawler sends to the service. For
example, in the case of HTTP \[[RFC9110](#RFC9110){.xref}\], the product
token [SHOULD]{.bcp14} be a substring in the User-Agent header. The
identification string [SHOULD]{.bcp14} describe the purpose of the
crawler. Here\'s an example of a User-Agent HTTP request header with a
link pointing to a page describing the purpose of the ExampleBot
crawler, which appears as a substring in the User-Agent HTTP header and
as a product token in the robots.txt user-agent
line:[¶](#section-2.2.1-1){.pilcrow}

[]{#name-example-of-a-user-agent-htt}

::: {#fig-1}
::: {#section-2.2.1-2.1 .alignCenter .art-text .artwork}
    +==========================================+========================+
    | User-Agent HTTP header                   | robots.txt user-agent  |
    |                                          | line                   |
    +==========================================+========================+
    | User-Agent: Mozilla/5.0 (compatible;     | user-agent: ExampleBot |
    | ExampleBot/0.1;                          |                        |
    | https://www.example.com/bot.html)        |                        |
    +------------------------------------------+------------------------+
:::

[Figure 1](#figure-1){.selfRef}: [Example of a User-Agent HTTP header
and robots.txt user-agent line for the ExampleBot product
token](#name-example-of-a-user-agent-htt){.selfRef}
:::

Note that the product token (ExampleBot) is a substring of the
User-Agent HTTP header.[¶](#section-2.2.1-3){.pilcrow}

Crawlers [MUST]{.bcp14} use case-insensitive matching to find the group
that matches the product token and then obey the rules of the group. If
there is more than one group matching the user-agent, the matching
groups\' rules [MUST]{.bcp14} be combined into one group and parsed
according to [Section
2.2.2](#the-allow-and-disallow-lines){.xref}.[¶](#section-2.2.1-4){.pilcrow}

[]{#name-example-of-how-to-merge-two}

::: {#fig-2}
::: {#section-2.2.1-5.1 .alignCenter .art-text .artwork}
    +========================================+========================+
    | Two groups that match the same product | Merged group           |
    | token exactly                          |                        |
    +========================================+========================+
    | user-agent: ExampleBot                 | user-agent: ExampleBot |
    | disallow: /foo                         | disallow: /foo         |
    | disallow: /bar                         | disallow: /bar         |
    |                                        | disallow: /baz         |
    | user-agent: ExampleBot                 |                        |
    | disallow: /baz                         |                        |
    +----------------------------------------+------------------------+
:::

[Figure 2](#figure-2){.selfRef}: [Example of how to merge two robots.txt
groups that match the same product
token](#name-example-of-how-to-merge-two){.selfRef}
:::

If no matching group exists, crawlers [MUST]{.bcp14} obey the group with
a user-agent line with the \"\*\" value, if
present.[¶](#section-2.2.1-6){.pilcrow}

[]{#name-example-of-no-matching-grou}

::: {#fig-3}
::: {#section-2.2.1-7.1 .alignCenter .art-text .artwork}
    +==================================+======================+
    | Two groups that don't explicitly | Applicable group for |
    | match ExampleBot                 | ExampleBot           |
    +==================================+======================+
    | user-agent: *                    | user-agent: *        |
    | disallow: /foo                   | disallow: /foo       |
    | disallow: /bar                   | disallow: /bar       |
    |                                  |                      |
    | user-agent: BazBot               |                      |
    | disallow: /baz                   |                      |
    +----------------------------------+----------------------+
:::

[Figure 3](#figure-3){.selfRef}: [Example of no matching groups other
than the \"\*\" for the ExampleBot product
token](#name-example-of-no-matching-grou){.selfRef}
:::

If no group matches the product token and there is no group with a
user-agent line with the \"\*\" value, or no groups are present at all,
no rules apply.[¶](#section-2.2.1-8){.pilcrow}
:::
:::

::: {#the-allow-and-disallow-lines}
::: {#section-2.2.2 .section}
#### [2.2.2.](#section-2.2.2){.section-number .selfRef} [The \"Allow\" and \"Disallow\" Lines](#name-the-allow-and-disallow-line){.section-name .selfRef} {#name-the-allow-and-disallow-line}

These lines indicate whether accessing a URI that matches the
corresponding path is allowed or
disallowed.[¶](#section-2.2.2-1){.pilcrow}

To evaluate if access to a URI is allowed, a crawler [MUST]{.bcp14}
match the paths in \"allow\" and \"disallow\" rules against the URI. The
matching [SHOULD]{.bcp14} be case sensitive. The matching [MUST]{.bcp14}
start with the first octet of the path. The most specific match found
[MUST]{.bcp14} be used. The most specific match is the match that has
the most octets. Duplicate rules in a group [MAY]{.bcp14} be
deduplicated. If an \"allow\" rule and a \"disallow\" rule are
equivalent, then the \"allow\" rule [SHOULD]{.bcp14} be used. If no
match is found amongst the rules in a group for a matching user-agent or
there are no rules in the group, the URI is allowed. The /robots.txt URI
is implicitly allowed.[¶](#section-2.2.2-2){.pilcrow}

Octets in the URI and robots.txt paths outside the range of the ASCII
coded character set, and those in the reserved range defined by
\[[RFC3986](#RFC3986){.xref}\], [MUST]{.bcp14} be percent-encoded as
defined by \[[RFC3986](#RFC3986){.xref}\] prior to
comparison.[¶](#section-2.2.2-3){.pilcrow}

If a percent-encoded ASCII octet is encountered in the URI, it
[MUST]{.bcp14} be unencoded prior to comparison, unless it is a reserved
character in the URI as defined by \[[RFC3986](#RFC3986){.xref}\] or the
character is outside the unreserved character range. The match evaluates
positively if and only if the end of the path from the rule is reached
before a difference in octets is
encountered.[¶](#section-2.2.2-4){.pilcrow}

For example:[¶](#section-2.2.2-5){.pilcrow}

[]{#name-examples-of-matching-percen}

::: {#fig-4}
::: {#section-2.2.2-6.1 .alignCenter .art-text .artwork}
    +==================+=======================+=======================+
    | Path             | Encoded Path          | Path to Match         |
    +==================+=======================+=======================+
    | /foo/bar?baz=quz | /foo/bar?baz=quz      | /foo/bar?baz=quz      |
    +------------------+-----------------------+-----------------------+
    | /foo/bar?baz=    | /foo/bar?baz=         | /foo/bar?baz=         |
    | https://foo.bar  | https%3A%2F%2Ffoo.bar | https%3A%2F%2Ffoo.bar |
    +------------------+-----------------------+-----------------------+
    | /foo/bar/        | /foo/bar/%E3%83%84    | /foo/bar/%E3%83%84    |
    | U+E38384         |                       |                       |
    +------------------+-----------------------+-----------------------+
    | /foo/            | /foo/bar/%E3%83%84    | /foo/bar/%E3%83%84    |
    | bar/%E3%83%84    |                       |                       |
    +------------------+-----------------------+-----------------------+
    | /foo/            | /foo/bar/%62%61%7A    | /foo/bar/baz          |
    | bar/%62%61%7A    |                       |                       |
    +------------------+-----------------------+-----------------------+
:::

[Figure 4](#figure-4){.selfRef}: [Examples of matching percent-encoded
URI components](#name-examples-of-matching-percen){.selfRef}
:::

The crawler [SHOULD]{.bcp14} ignore \"disallow\" and \"allow\" rules
that are not in any group (for example, any rule that precedes the first
user-agent line).[¶](#section-2.2.2-7){.pilcrow}

Implementors [MAY]{.bcp14} bridge encoding mismatches if they detect
that the robots.txt file is not UTF-8
encoded.[¶](#section-2.2.2-8){.pilcrow}
:::
:::

::: {#special-characters}
::: {#section-2.2.3 .section}
#### [2.2.3.](#section-2.2.3){.section-number .selfRef} [Special Characters](#name-special-characters){.section-name .selfRef} {#name-special-characters}

Crawlers [MUST]{.bcp14} support the following special
characters:[¶](#section-2.2.3-1){.pilcrow}

[]{#name-list-of-special-characters-}

::: {#fig-5}
::: {#section-2.2.3-2.1 .alignCenter .art-text .artwork}
    +===========+===================+==============================+
    | Character | Description       | Example                      |
    +===========+===================+==============================+
    | #         | Designates a line | allow: / # comment in line   |
    |           | comment.          |                              |
    |           |                   | # comment on its own line    |
    +-----------+-------------------+------------------------------+
    | $         | Designates the    | allow: /this/path/exactly$   |
    |           | end of the match  |                              |
    |           | pattern.          |                              |
    +-----------+-------------------+------------------------------+
    | *         | Designates 0 or   | allow: /this/*/exactly       |
    |           | more instances of |                              |
    |           | any character.    |                              |
    +-----------+-------------------+------------------------------+
:::

[Figure 5](#figure-5){.selfRef}: [List of special characters in
robots.txt files](#name-list-of-special-characters-){.selfRef}
:::

If crawlers match special characters verbatim in the URI, crawlers
[SHOULD]{.bcp14} use \"%\" encoding. For
example:[¶](#section-2.2.3-3){.pilcrow}

[]{#name-example-of-percent-encoding}

::: {#fig-6}
::: {#section-2.2.3-4.1 .alignCenter .art-text .artwork}
    +============================+====================================+
    | Percent-encoded Pattern    | URI                                |
    +============================+====================================+
    | /path/file-with-a-%2A.html | https://www.example.com/path/      |
    |                            | file-with-a-*.html                 |
    +----------------------------+------------------------------------+
    | /path/foo-%24              | https://www.example.com/path/foo-$ |
    +----------------------------+------------------------------------+
:::

[Figure 6](#figure-6){.selfRef}: [Example of
percent-encoding](#name-example-of-percent-encoding){.selfRef}
:::
:::
:::

::: {#other-records}
::: {#section-2.2.4 .section}
#### [2.2.4.](#section-2.2.4){.section-number .selfRef} [Other Records](#name-other-records){.section-name .selfRef} {#name-other-records}

Crawlers [MAY]{.bcp14} interpret other records that are not part of the
robots.txt protocol \-- for example, \"Sitemaps\"
\[[SITEMAPS](#SITEMAPS){.xref}\]. Crawlers [MAY]{.bcp14} be lenient when
interpreting other records. For example, crawlers may accept common
misspellings of the record.[¶](#section-2.2.4-1){.pilcrow}

Parsing of other records [MUST NOT]{.bcp14} interfere with the parsing
of explicitly defined records in [Section 2](#specification){.xref}. For
example, a \"Sitemaps\" record [MUST NOT]{.bcp14} terminate a
group.[¶](#section-2.2.4-2){.pilcrow}
:::
:::
:::
:::

::: {#access-method}
::: {#section-2.3 .section}
### [2.3.](#section-2.3){.section-number .selfRef} [Access Method](#name-access-method){.section-name .selfRef} {#name-access-method}

The rules [MUST]{.bcp14} be accessible in a file named \"/robots.txt\"
(all lowercase) in the top-level path of the service. The file
[MUST]{.bcp14} be UTF-8 encoded (as defined in
\[[RFC3629](#RFC3629){.xref}\]) and Internet Media Type \"text/plain\"
(as defined in
\[[RFC2046](#RFC2046){.xref}\]).[¶](#section-2.3-1){.pilcrow}

As per \[[RFC3986](#RFC3986){.xref}\], the URI of the robots.txt file
is:[¶](#section-2.3-2){.pilcrow}

\"scheme:\[//authority\]/robots.txt\"[¶](#section-2.3-3){.pilcrow}

For example, in the context of HTTP or FTP, the URI
is:[¶](#section-2.3-4){.pilcrow}

::: {#section-2.3-5 .alignLeft .art-text .artwork}
              https://www.example.com/robots.txt

              ftp://ftp.example.com/robots.txt

[¶](#section-2.3-5){.pilcrow}
:::

::: {#access-results}
::: {#section-2.3.1 .section}
#### [2.3.1.](#section-2.3.1){.section-number .selfRef} [Access Results](#name-access-results){.section-name .selfRef} {#name-access-results}

::: {#successful-access}
::: {#section-2.3.1.1 .section}
##### [2.3.1.1.](#section-2.3.1.1){.section-number .selfRef} [Successful Access](#name-successful-access){.section-name .selfRef} {#name-successful-access}

If the crawler successfully downloads the robots.txt file, the crawler
[MUST]{.bcp14} follow the parseable
rules.[¶](#section-2.3.1.1-1){.pilcrow}
:::
:::

::: {#redirects}
::: {#section-2.3.1.2 .section}
##### [2.3.1.2.](#section-2.3.1.2){.section-number .selfRef} [Redirects](#name-redirects){.section-name .selfRef} {#name-redirects}

It\'s possible that a server responds to a robots.txt fetch request with
a redirect, such as HTTP 301 or HTTP 302 in the case of HTTP. The
crawlers [SHOULD]{.bcp14} follow at least five consecutive redirects,
even across authorities (for example, hosts in the case of
HTTP).[¶](#section-2.3.1.2-1){.pilcrow}

If a robots.txt file is reached within five consecutive redirects, the
robots.txt file [MUST]{.bcp14} be fetched, parsed, and its rules
followed in the context of the initial
authority.[¶](#section-2.3.1.2-2){.pilcrow}

If there are more than five consecutive redirects, crawlers
[MAY]{.bcp14} assume that the robots.txt file is
unavailable.[¶](#section-2.3.1.2-3){.pilcrow}
:::
:::

::: {#unavailable-status}
::: {#section-2.3.1.3 .section}
##### [2.3.1.3.](#section-2.3.1.3){.section-number .selfRef} [\"Unavailable\" Status](#name-unavailable-status){.section-name .selfRef} {#name-unavailable-status}

\"Unavailable\" means the crawler tries to fetch the robots.txt file and
the server responds with status codes indicating that the resource in
question is unavailable. For example, in the context of HTTP, such
status codes are in the 400-499 range.[¶](#section-2.3.1.3-1){.pilcrow}

If a server status code indicates that the robots.txt file is
unavailable to the crawler, then the crawler [MAY]{.bcp14} access any
resources on the server.[¶](#section-2.3.1.3-2){.pilcrow}
:::
:::

::: {#unreachable-status}
::: {#section-2.3.1.4 .section}
##### [2.3.1.4.](#section-2.3.1.4){.section-number .selfRef} [\"Unreachable\" Status](#name-unreachable-status){.section-name .selfRef} {#name-unreachable-status}

If the robots.txt file is unreachable due to server or network errors,
this means the robots.txt file is undefined and the crawler
[MUST]{.bcp14} assume complete disallow. For example, in the context of
HTTP, server errors are identified by status codes in the 500-599
range.[¶](#section-2.3.1.4-1){.pilcrow}

If the robots.txt file is undefined for a reasonably long period of time
(for example, 30 days), crawlers [MAY]{.bcp14} assume that the
robots.txt file is unavailable as defined in [Section
2.3.1.3](#unavailable-status){.xref} or continue to use a cached
copy.[¶](#section-2.3.1.4-2){.pilcrow}
:::
:::

::: {#parsing-errors}
::: {#section-2.3.1.5 .section}
##### [2.3.1.5.](#section-2.3.1.5){.section-number .selfRef} [Parsing Errors](#name-parsing-errors){.section-name .selfRef} {#name-parsing-errors}

Crawlers [MUST]{.bcp14} try to parse each line of the robots.txt file.
Crawlers [MUST]{.bcp14} use the parseable
rules.[¶](#section-2.3.1.5-1){.pilcrow}
:::
:::
:::
:::
:::
:::

::: {#caching}
::: {#section-2.4 .section}
### [2.4.](#section-2.4){.section-number .selfRef} [Caching](#name-caching){.section-name .selfRef} {#name-caching}

Crawlers [MAY]{.bcp14} cache the fetched robots.txt file\'s contents.
Crawlers [MAY]{.bcp14} use standard cache control as defined in
\[[RFC9111](#RFC9111){.xref}\]. Crawlers [SHOULD NOT]{.bcp14} use the
cached version for more than 24 hours, unless the robots.txt file is
unreachable.[¶](#section-2.4-1){.pilcrow}
:::
:::

::: {#limits}
::: {#section-2.5 .section}
### [2.5.](#section-2.5){.section-number .selfRef} [Limits](#name-limits){.section-name .selfRef} {#name-limits}

Crawlers [SHOULD]{.bcp14} impose a parsing limit to protect their
systems; see [Section 3](#security){.xref}. The parsing limit
[MUST]{.bcp14} be at least 500 kibibytes
\[[KiB](#KiB){.xref}\].[¶](#section-2.5-1){.pilcrow}
:::
:::
:::
:::

::: {#security}
::: {#section-3 .section}
## [3.](#section-3){.section-number .selfRef} [Security Considerations](#name-security-considerations){.section-name .selfRef} {#name-security-considerations}

The Robots Exclusion Protocol is not a substitute for valid content
security measures. Listing paths in the robots.txt file exposes them
publicly and thus makes the paths discoverable. To control access to the
URI paths in a robots.txt file, users of the protocol should employ a
valid security measure relevant to the application layer on which the
robots.txt file is served \-- for example, in the case of HTTP, HTTP
Authentication as defined in
\[[RFC9110](#RFC9110){.xref}\].[¶](#section-3-1){.pilcrow}

To protect against attacks against their system, implementors of
robots.txt parsing and matching logic should take the following
considerations into account:[¶](#section-3-2){.pilcrow}

[]{.break}

Memory management:
:   [Section 2.5](#limits){.xref} defines the lower limit of bytes that
    must be processed, which inherently also protects the parser from
    out-of-memory scenarios.[¶](#section-3-3.2){.pilcrow}
:   

Invalid characters:
:   [Section 2.2](#formal-syntax){.xref} defines a set of characters
    that parsers and matchers can expect in robots.txt files.
    Out-of-bound characters should be rejected as invalid, which limits
    the available attack vectors that attempt to compromise the
    system.[¶](#section-3-3.4){.pilcrow}
:   

Untrusted content:
:   Implementors should treat the content of a robots.txt file as
    untrusted content, as defined by the specification of the
    application layer used. For example, in the context of HTTP,
    implementors should follow the Security Considerations section of
    \[[RFC9110](#RFC9110){.xref}\].[¶](#section-3-3.6){.pilcrow}
:   
:::
:::

::: {#IANA}
::: {#section-4 .section}
## [4.](#section-4){.section-number .selfRef} [IANA Considerations](#name-iana-considerations){.section-name .selfRef} {#name-iana-considerations}

This document has no IANA actions.[¶](#section-4-1){.pilcrow}
:::
:::

::: {#examples}
::: {#section-5 .section}
## [5.](#section-5){.section-number .selfRef} [Examples](#name-examples){.section-name .selfRef} {#name-examples}

::: {#simple-example}
::: {#section-5.1 .section}
### [5.1.](#section-5.1){.section-number .selfRef} [Simple Example](#name-simple-example){.section-name .selfRef} {#name-simple-example}

The following example shows:[¶](#section-5.1-1){.pilcrow}

[]{.break}

\*:
:   A group that\'s relevant to all user agents that don\'t have an
    explicitly defined matching group. It allows access to the URLs with
    the /publications/ path prefix, and it restricts access to the URLs
    with the /example/ path prefix and to all URLs with a .gif suffix.
    The \"\*\" character designates any character, including the
    otherwise-required forward slash; see [Section
    2.2](#formal-syntax){.xref}.[¶](#section-5.1-2.2){.pilcrow}
:   

foobot:
:   A regular case. A single user agent followed by rules. The crawler
    only has access to two URL path prefixes on the site \--
    /example/page.html and /example/allowed.gif. The rules of the group
    are missing the optional space character, which is acceptable as
    defined in [Section
    2.2](#formal-syntax){.xref}.[¶](#section-5.1-2.4){.pilcrow}
:   

barbot and bazbot:
:   A group that\'s relevant for more than one user agent. The crawlers
    are not allowed to access the URLs with the /example/page.html path
    prefix but otherwise have unrestricted access to the rest of the
    URLs on the site.[¶](#section-5.1-2.6){.pilcrow}
:   

quxbot:
:   An empty group at the end of the file. The crawler has unrestricted
    access to the URLs on the site.[¶](#section-5.1-2.8){.pilcrow}
:   

::: {#section-5.1-3 .alignLeft .art-text .artwork}
                User-Agent: *
                Disallow: *.gif$
                Disallow: /example/
                Allow: /publications/

                User-Agent: foobot
                Disallow:/
                Allow:/example/page.html
                Allow:/example/allowed.gif

                User-Agent: barbot
                User-Agent: bazbot
                Disallow: /example/page.html

                User-Agent: quxbot

                EOF

[¶](#section-5.1-3){.pilcrow}
:::
:::
:::

::: {#longest-match}
::: {#section-5.2 .section}
### [5.2.](#section-5.2){.section-number .selfRef} [Longest Match](#name-longest-match){.section-name .selfRef} {#name-longest-match}

The following example shows that in the case of two rules, the longest
one is used for matching. In the following case,
/example/page/disallowed.gif [MUST]{.bcp14} be used for the URI
example.com/example/page/disallow.gif.[¶](#section-5.2-1){.pilcrow}

::: {#section-5.2-2 .alignLeft .art-text .artwork}
                User-Agent: foobot
                Allow: /example/page/
                Disallow: /example/page/disallowed.gif

[¶](#section-5.2-2){.pilcrow}
:::
:::
:::
:::
:::

::: {#section-6 .section}
## [6.](#section-6){.section-number .selfRef} [References](#name-references){.section-name .selfRef} {#name-references}

::: {#section-6.1 .section}
### [6.1.](#section-6.1){.section-number .selfRef} [Normative References](#name-normative-references){.section-name .selfRef} {#name-normative-references}

\[RFC2046\]
:   [Freed, N.]{.refAuthor} and [N. Borenstein]{.refAuthor},
    [\"Multipurpose Internet Mail Extensions (MIME) Part Two: Media
    Types\"]{.refTitle}, [RFC 2046]{.seriesInfo}, [DOI
    10.17487/RFC2046]{.seriesInfo}, November 1996,
    \<<https://www.rfc-editor.org/info/rfc2046>\>.
:   

\[RFC2119\]
:   [Bradner, S.]{.refAuthor}, [\"Key words for use in RFCs to Indicate
    Requirement Levels\"]{.refTitle}, [BCP 14]{.seriesInfo}, [RFC
    2119]{.seriesInfo}, [DOI 10.17487/RFC2119]{.seriesInfo}, March 1997,
    \<<https://www.rfc-editor.org/info/rfc2119>\>.
:   

\[RFC3629\]
:   [Yergeau, F.]{.refAuthor}, [\"UTF-8, a transformation format of ISO
    10646\"]{.refTitle}, [STD 63]{.seriesInfo}, [RFC 3629]{.seriesInfo},
    [DOI 10.17487/RFC3629]{.seriesInfo}, November 2003,
    \<<https://www.rfc-editor.org/info/rfc3629>\>.
:   

\[RFC3986\]
:   [Berners-Lee, T.]{.refAuthor}, [Fielding, R.]{.refAuthor}, and [L.
    Masinter]{.refAuthor}, [\"Uniform Resource Identifier (URI): Generic
    Syntax\"]{.refTitle}, [STD 66]{.seriesInfo}, [RFC
    3986]{.seriesInfo}, [DOI 10.17487/RFC3986]{.seriesInfo}, January
    2005, \<<https://www.rfc-editor.org/info/rfc3986>\>.
:   

\[RFC5234\]
:   [Crocker, D., Ed.]{.refAuthor} and [P. Overell]{.refAuthor},
    [\"Augmented BNF for Syntax Specifications: ABNF\"]{.refTitle}, [STD
    68]{.seriesInfo}, [RFC 5234]{.seriesInfo}, [DOI
    10.17487/RFC5234]{.seriesInfo}, January 2008,
    \<<https://www.rfc-editor.org/info/rfc5234>\>.
:   

\[RFC8174\]
:   [Leiba, B.]{.refAuthor}, [\"Ambiguity of Uppercase vs Lowercase in
    RFC 2119 Key Words\"]{.refTitle}, [BCP 14]{.seriesInfo}, [RFC
    8174]{.seriesInfo}, [DOI 10.17487/RFC8174]{.seriesInfo}, May 2017,
    \<<https://www.rfc-editor.org/info/rfc8174>\>.
:   

\[RFC8288\]
:   [Nottingham, M.]{.refAuthor}, [\"Web Linking\"]{.refTitle}, [RFC
    8288]{.seriesInfo}, [DOI 10.17487/RFC8288]{.seriesInfo}, October
    2017, \<<https://www.rfc-editor.org/info/rfc8288>\>.
:   

\[RFC9110\]
:   [Fielding, R., Ed.]{.refAuthor}, [Nottingham, M., Ed.]{.refAuthor},
    and [J. Reschke, Ed.]{.refAuthor}, [\"HTTP Semantics\"]{.refTitle},
    [STD 97]{.seriesInfo}, [RFC 9110]{.seriesInfo}, [DOI
    10.17487/RFC9110]{.seriesInfo}, June 2022,
    \<<https://www.rfc-editor.org/info/rfc9110>\>.
:   

\[RFC9111\]
:   [Fielding, R., Ed.]{.refAuthor}, [Nottingham, M., Ed.]{.refAuthor},
    and [J. Reschke, Ed.]{.refAuthor}, [\"HTTP Caching\"]{.refTitle},
    [STD 98]{.seriesInfo}, [RFC 9111]{.seriesInfo}, [DOI
    10.17487/RFC9111]{.seriesInfo}, June 2022,
    \<<https://www.rfc-editor.org/info/rfc9111>\>.
:   
:::

::: {#section-6.2 .section}
### [6.2.](#section-6.2){.section-number .selfRef} [Informative References](#name-informative-references){.section-name .selfRef} {#name-informative-references}

\[KiB\]
:   [\"Kibibyte\"]{.refTitle}, [Simple English Wikipedia, the free
    encyclopedia]{.refContent}, 17 September 2020,
    \<<https://simple.wikipedia.org/wiki/Kibibyte>\>.
:   

\[ROBOTSTXT\]
:   [\"The Web Robots Pages (including /robots.txt)\"]{.refTitle}, 2007,
    \<<https://www.robotstxt.org/>\>.
:   

\[SITEMAPS\]
:   [\"What are Sitemaps? (Sitemap protocol)\"]{.refTitle}, April 2020,
    \<<https://www.sitemaps.org/index.html>\>.
:   
:::
:::

::: {#authors-addresses}
::: {#appendix-A .section}
## [Authors\' Addresses](#name-authors-addresses){.section-name .selfRef} {#name-authors-addresses}

::: {.left dir="auto"}
[Martijn Koster]{.fn .nameRole}
:::

::: {.left dir="auto"}
[Stalworthy Manor Farm]{.extended-address}
:::

::: {.left dir="auto"}
[Suton Lane]{.street-address}
:::

::: {.left dir="auto"}
[Wymondham, Norfolk]{.locality}
:::

::: {.left dir="auto"}
[NR18 9JG]{.postal-code}
:::

::: {.left dir="auto"}
[United Kingdom]{.country-name}
:::

::: email
Email: <m.koster@greenhills.co.uk>
:::

::: {.left dir="auto"}
[Gary Illyes]{.fn .nameRole}
:::

::: {.left dir="auto"}
[Google LLC]{.org}
:::

::: {.left dir="auto"}
[Brandschenkestrasse 110]{.street-address}
:::

::: {.left dir="auto"}
CH-[8002]{.postal-code} [Zürich]{.locality}
:::

::: {.left dir="auto"}
[Switzerland]{.country-name}
:::

::: email
Email: <garyillyes@google.com>
:::

::: {.left dir="auto"}
[Henner Zeller]{.fn .nameRole}
:::

::: {.left dir="auto"}
[Google LLC]{.org}
:::

::: {.left dir="auto"}
[1600 Amphitheatre Pkwy]{.street-address}
:::

::: {.left dir="auto"}
[Mountain View]{.locality}, [CA]{.region} [94043]{.postal-code}
:::

::: {.left dir="auto"}
[United States of America]{.country-name}
:::

::: email
Email: <henner@google.com>
:::

::: {.left dir="auto"}
[Lizzi Sassman]{.fn .nameRole}
:::

::: {.left dir="auto"}
[Google LLC]{.org}
:::

::: {.left dir="auto"}
[Brandschenkestrasse 110]{.street-address}
:::

::: {.left dir="auto"}
CH-[8002]{.postal-code} [Zürich]{.locality}
:::

::: {.left dir="auto"}
[Switzerland]{.country-name}
:::

::: email
Email: <lizzi@google.com>
:::
:::
:::
